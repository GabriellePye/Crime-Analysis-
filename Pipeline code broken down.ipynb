{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05a9ab5b",
   "metadata": {},
   "source": [
    "<h1>Data Pipeline</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0439ec",
   "metadata": {},
   "source": [
    "Setup and Imports: set up necessary libraries and logging configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22a1e78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from geopy.geocoders import Nominatim\n",
    "import json\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, # Levels above debug\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    filename='data_pipeline.log', # Pipeline log file\n",
    "    filemode='w' # Overwrite logs\n",
    ")\n",
    "\n",
    "# Initialize geocoder\n",
    "geolocator = Nominatim(user_agent='myGeocoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b3ad10",
   "metadata": {},
   "source": [
    "Database Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d49b983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "DATA_FOLDER = './Data/' # Paths to each dataset file and their types are defined.\n",
    "STAGING_FOLDER = './staging'\n",
    "PRIMARY_FOLDER = './primary'\n",
    "POLICE_DATA_FOLDER = os.path.join(DATA_FOLDER, 'Police Dataset 2021-2024')\n",
    "REPORTING_FOLDER = './reporting'\n",
    "CACHE_FILE = 'postcode_cache.json'\n",
    "\n",
    "STAGED_FILES = {\n",
    "    'staged_police_data': 'staged_police_data.csv',\n",
    "    'staged_cbp_8322_authority': 'staged_CBP-8322-authority.csv',\n",
    "    'staged_cbp_8322_constituency': 'staged_CBP-8322-constituency.csv',\n",
    "    'staged_cbp_7293': 'staged_CBP-7293.csv',\n",
    "    'staged_english_la_name_codes': 'staged_EnglishLaNameCodes.csv',\n",
    "    'staged_house_prices': 'staged_house_prices.csv'\n",
    "}\n",
    "\n",
    "PRIMARY_FILES = {\n",
    "    'staged_police_data': 'primary_police_data.csv',\n",
    "    'staged_cbp_8322_authority': 'primary_CBP-8322-authority.csv',\n",
    "    'staged_cbp_8322_constituency': 'primary_CBP-8322-constituency.csv',\n",
    "    'staged_cbp_7293': 'primary_CBP-7293.csv',\n",
    "    'staged_english_la_name_codes': 'primary_EnglishLaNameCodes.csv',\n",
    "    'staged_house_prices': 'primary_house_prices.csv'\n",
    "}\n",
    "\n",
    "REPORTING_FILES = {\n",
    "    'primary_police_data': 'reporting_police_data.csv',\n",
    "    'primary_CBP-8322-authority': 'reporting_CBP-8322-authority.csv',\n",
    "    'primary_CBP-8322-constituency': 'reporting_CBP-8322-constituency.csv',\n",
    "    'primary_CBP-7293': 'reporting_CBP-7293.csv',\n",
    "    'primary_EnglishLaNameCodes': 'reporting_EnglishLaNameCodes.csv',\n",
    "    'primary_house_prices': 'reporting_house_prices.csv'\n",
    "}\n",
    "\n",
    "\n",
    "# Define dataset file paths and types\n",
    "CBP_8322_AUTHORITY_FILE = os.path.join(DATA_FOLDER, 'CBP-8322-authority.xlsx')\n",
    "CBP_8322_CONSTITUENCY_FILE = os.path.join(DATA_FOLDER, 'CBP-8322-constituency.xlsx')\n",
    "CBP_7293_FILE = os.path.join(DATA_FOLDER, 'CBP-7293.xlsx')\n",
    "ENGLISH_LA_NAME_CODES_FILE = os.path.join(DATA_FOLDER, 'EnglishLaNameCodes.csv')\n",
    "HOUSE_PRICES_FILE = os.path.join(DATA_FOLDER, 'house_prices.csv')\n",
    "POLICE_DATA_FOLDER = os.path.join(DATA_FOLDER, 'Police Dataset 2021-2024')\n",
    "\n",
    "# Define file types\n",
    "FILE_TYPES = {\n",
    "    'CBP-8322-authority': 'excel',\n",
    "    'CBP-8322-constituency': 'excel',\n",
    "    'CBP-7293': 'excel',\n",
    "    'EnglishLaNameCodes': 'csv',\n",
    "    'house_prices': 'csv'\n",
    "}\n",
    "\n",
    "def load_data(file_path: str, file_type: str) -> pd.DataFrame: # Loads individual datasets based on file typ\n",
    "    \"\"\"Load a dataset from a given file path and type (csv or excel).\"\"\"\n",
    "    try:\n",
    "        if file_type == 'csv':\n",
    "            df = pd.read_csv(file_path)\n",
    "        elif file_type == 'excel':\n",
    "            df = pd.read_excel(file_path)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file type. Only 'csv' and 'excel' are supported.\")\n",
    "        logging.info(f\"Successfully loaded {file_type} file: {file_path}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading {file_type} file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_police_data(police_folder_path: str) -> pd.DataFrame: # Concatenates multiple police datasets into one DataFrame\n",
    "    \"\"\"Load and concatenate police datasets.\"\"\"\n",
    "    df_list = []\n",
    "    for root, dirs, files in os.walk(police_folder_path):\n",
    "        for file_name in files:\n",
    "            if file_name.endswith('.csv'):\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                df = load_data(file_path, 'csv')\n",
    "                if df is not None:\n",
    "                    df_list.append(df)\n",
    "    \n",
    "    if df_list:\n",
    "        combined_df = pd.concat(df_list, ignore_index=True)\n",
    "        logging.info(\"Successfully concatenated police datasets.\")\n",
    "        return combined_df\n",
    "    else:\n",
    "        logging.error(\"No valid police datasets found to concatenate.\")\n",
    "        return None\n",
    "\n",
    "def load_all_datasets(): # Uses constants to load all datasets and handle the police data separately.\n",
    "    \"\"\"Load all datasets using predefined constants.\"\"\"\n",
    "    files_and_types = {\n",
    "        'CBP-8322-authority': (CBP_8322_AUTHORITY_FILE, FILE_TYPES['CBP-8322-authority']),\n",
    "        'CBP-8322-constituency': (CBP_8322_CONSTITUENCY_FILE, FILE_TYPES['CBP-8322-constituency']),\n",
    "        'CBP-7293': (CBP_7293_FILE, FILE_TYPES['CBP-7293']),\n",
    "        'EnglishLaNameCodes': (ENGLISH_LA_NAME_CODES_FILE, FILE_TYPES['EnglishLaNameCodes']),\n",
    "        'house_prices': (HOUSE_PRICES_FILE, FILE_TYPES['house_prices'])\n",
    "    }\n",
    "    \n",
    "    datasets = {}\n",
    "    for name, (file_path, file_type) in files_and_types.items():\n",
    "        df = load_data(file_path, file_type)\n",
    "        if df is not None:\n",
    "            datasets[name] = df\n",
    "    \n",
    "    # Load police data separately\n",
    "    police_df = load_police_data(POLICE_DATA_FOLDER)\n",
    "    if police_df is not None:\n",
    "        datasets['police'] = police_df\n",
    "    \n",
    "    return datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7330ae",
   "metadata": {},
   "source": [
    "Setting up the Functions: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e529f019",
   "metadata": {},
   "source": [
    "Data Standardization: Normalize column names and standardize text values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6b5bdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Standardize text columns by converting to lowercase and stripping whitespace.\"\"\"\n",
    "    \n",
    "    # Normalize column names (lowercase and replace spaces with underscores)\n",
    "    df.columns = [col.lower().replace(' ', '_') for col in df.columns]\n",
    "    \n",
    "    # Identify object (string) columns dynamically\n",
    "    string_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    # If there are no string columns, log and skip processing\n",
    "    if not string_columns:\n",
    "        logging.info(\"No string columns found for standardization.\")\n",
    "        return df\n",
    "    \n",
    "    # Standardize string columns (convert to lowercase, strip whitespace)\n",
    "    for col in string_columns:\n",
    "        logging.info(f\"Standardizing column: {col}\")\n",
    "        df[col] = df[col].astype(str).str.lower().str.strip()\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c836bfed",
   "metadata": {},
   "source": [
    "Duplicate Removal: Remove duplicate rows based on values across all columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4515cc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Remove duplicate rows based on all columns if duplicates exist.\"\"\"\n",
    "    try:\n",
    "        # Check for duplicates\n",
    "        if df.duplicated().any():\n",
    "            # Drop duplicate rows across all columns\n",
    "            df = df.drop_duplicates()\n",
    "            logging.info(\"Duplicates removed successfully.\")\n",
    "        else:\n",
    "            logging.info(\"No duplicates found; skipping removal.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error removing duplicates: {e}\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402f1e16",
   "metadata": {},
   "source": [
    "Date Handling: Convert columns with date-like data into datetime objects and extract additional time features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "410dc4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_dates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Convert columns with date-like data into datetime and create month/year columns if applicable.\"\"\"\n",
    "    \n",
    "    # Identify date columns: columns that contain 'date' in their name or have date-like values\n",
    "    date_columns = [col for col in df.columns if 'date' in col.lower() or df[col].astype(str).str.contains(r'\\d{2}/\\d{2}/\\d{4}', na=False).any()]\n",
    "    \n",
    "    # If no date columns are found, log and return the DataFrame unchanged\n",
    "    if not date_columns:\n",
    "        logging.info(\"No date-like columns found.\")\n",
    "        return df\n",
    "    \n",
    "    logging.info(f\"Converting date columns: {date_columns}\")\n",
    "    \n",
    "    for col in date_columns:\n",
    "        # Convert column to datetime\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "\n",
    "        # Create 'month' and 'year' columns based on the datetime column\n",
    "        df[f'{col}_month'] = df[col].dt.month\n",
    "        df[f'{col}_year'] = df[col].dt.year\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ff03e7",
   "metadata": {},
   "source": [
    "Column Pruning: Drop columns that are entirely empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ebf6de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_empty_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Drop columns that are entirely empty if any exist.\"\"\"\n",
    "    # Identify columns that are entirely empty\n",
    "    empty_cols = df.columns[df.isna().all()].tolist()\n",
    "    \n",
    "    if empty_cols:\n",
    "        logging.info(f\"Dropping empty columns: {empty_cols}\")\n",
    "        df = df.drop(columns=empty_cols)\n",
    "    else:\n",
    "        logging.info(\"No empty columns to drop.\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58109a72",
   "metadata": {},
   "source": [
    "Missing Values Handling: Fill missing values with median or mean values for numerical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b19b30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Handle missing values by filling numerical columns with median values if necessary.\"\"\"\n",
    "    # Identify numerical columns\n",
    "    numerical_cols = df.select_dtypes(include=['number']).columns\n",
    "    \n",
    "    # Check if there are any numerical columns with missing values\n",
    "    missing_data = df[numerical_cols].isna().any().any()\n",
    "    \n",
    "    if not missing_data:\n",
    "        # If no missing values are found in numerical columns, skip processing\n",
    "        logging.info(\"No missing values found in numerical columns. Skipping missing values handling.\")\n",
    "        return df\n",
    "    \n",
    "    # If there are missing values, handle them\n",
    "    for col in numerical_cols:\n",
    "        if df[col].isna().any():\n",
    "            logging.info(f\"Handling missing values for column: {col}\")\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cd63ee",
   "metadata": {},
   "source": [
    "Outlier Removal: Filter outliers based on latitude and longitude ranges. This is specific to this pipeline as we are looking for areas within the South East region."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a806f284",
   "metadata": {},
   "source": [
    "To ensure that the latitude and longitude values fall within the specified ranges:\n",
    "\n",
    "Latitude should be between 50 and 52\n",
    "Longitude should be between -2 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc60029f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df: pd.DataFrame, lat_range: tuple = (50, 52), lon_range: tuple = (-2, 2)) -> pd.DataFrame:\n",
    "    \"\"\"Remove outliers based on latitude and longitude ranges if necessary.\"\"\"\n",
    "    if 'latitude' in df.columns and 'longitude' in df.columns:\n",
    "        # Check if any latitude or longitude values are outside the specified ranges\n",
    "        outlier_lat = df['latitude'].notna() & ~df['latitude'].between(lat_range[0], lat_range[1])\n",
    "        outlier_lon = df['longitude'].notna() & ~df['longitude'].between(lon_range[0], lon_range[1])\n",
    "        \n",
    "        if outlier_lat.any() or outlier_lon.any():\n",
    "            logging.info(\"Removing outliers based on latitude and longitude.\")\n",
    "            original_size = len(df)\n",
    "            df = df[~(outlier_lat | outlier_lon)]\n",
    "            new_size = len(df)\n",
    "            logging.info(f\"Removed {original_size - new_size} rows with out-of-range lat/long values.\")\n",
    "        else:\n",
    "            logging.info(\"No outliers found in latitude/longitude columns. Skipping outlier removal.\")\n",
    "    else:\n",
    "        logging.info(\"No latitude/longitude columns found. Skipping outlier removal.\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee3e641",
   "metadata": {},
   "source": [
    "Staging: Loading the datasets and concatenating the Police Dataset 2021-2024. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "361256c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GabriellePye\\AppData\\Local\\Temp\\ipykernel_18044\\426225891.py:18: DtypeWarning: Columns (17,18,19,20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def create_staging_folder():\n",
    "    \"\"\"Create the staging folder if it does not exist.\"\"\"\n",
    "    if not os.path.exists(STAGING_FOLDER):\n",
    "        os.makedirs(STAGING_FOLDER)\n",
    "        logging.info(f\"Created staging folder: {STAGING_FOLDER}\")\n",
    "    else:\n",
    "        logging.info(f\"Staging folder already exists: {STAGING_FOLDER}\")\n",
    "\n",
    "def load_data(file_path: str, file_type: str) -> pd.DataFrame:\n",
    "    \"\"\"Load a dataset from a given file path and type (csv or excel).\"\"\"\n",
    "    try:\n",
    "        if file_type == 'csv':\n",
    "            df = pd.read_csv(file_path)\n",
    "        elif file_type == 'excel':\n",
    "            df = pd.read_excel(file_path)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file type. Only 'csv' and 'excel' are supported.\")\n",
    "        logging.info(f\"Successfully loaded {file_type} file: {file_path}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading {file_type} file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_police_data(police_folder_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load and concatenate police datasets.\"\"\"\n",
    "    df_list = []\n",
    "    for root, dirs, files in os.walk(police_folder_path):\n",
    "        for file_name in files:\n",
    "            if file_name.endswith('.csv'):\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                df = load_data(file_path, 'csv')\n",
    "                if df is not None:\n",
    "                    df_list.append(df)\n",
    "    \n",
    "    if df_list:\n",
    "        combined_df = pd.concat(df_list, ignore_index=True)\n",
    "        logging.info(\"Successfully concatenated police datasets.\")\n",
    "        return combined_df\n",
    "    else:\n",
    "        logging.error(\"No valid police datasets found to concatenate.\")\n",
    "        return None\n",
    "\n",
    "def save_to_csv(df: pd.DataFrame, file_path: str):\n",
    "    \"\"\"Save a DataFrame to a CSV file.\"\"\"\n",
    "    try:\n",
    "        df.to_csv(file_path, index=False)\n",
    "        logging.info(f\"Successfully saved file: {file_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving file {file_path}: {e}\")\n",
    "\n",
    "def staging():\n",
    "    \"\"\"Load datasets, concatenate police data, and save as staged files.\"\"\"\n",
    "    create_staging_folder()\n",
    "    \n",
    "    # Load police data\n",
    "    police_df = load_police_data(POLICE_DATA_FOLDER)\n",
    "    if police_df is not None:\n",
    "        save_to_csv(police_df, os.path.join(STAGING_FOLDER, 'staged_police_data.csv'))\n",
    "\n",
    "    # Load other datasets\n",
    "    files_and_types = {\n",
    "        'CBP-8322-authority': (CBP_8322_AUTHORITY_FILE, FILE_TYPES['CBP-8322-authority']),\n",
    "        'CBP-8322-constituency': (CBP_8322_CONSTITUENCY_FILE, FILE_TYPES['CBP-8322-constituency']),\n",
    "        'CBP-7293': (CBP_7293_FILE, FILE_TYPES['CBP-7293']),\n",
    "        'EnglishLaNameCodes': (ENGLISH_LA_NAME_CODES_FILE, FILE_TYPES['EnglishLaNameCodes']),\n",
    "        'house_prices': (HOUSE_PRICES_FILE, FILE_TYPES['house_prices'])\n",
    "    }\n",
    "    \n",
    "    for name, (file_path, file_type) in files_and_types.items():\n",
    "        df = load_data(file_path, file_type)\n",
    "        if df is not None:\n",
    "            save_to_csv(df, os.path.join(STAGING_FOLDER, f'staged_{name}.csv'))\n",
    "\n",
    "# Run the staging function\n",
    "if __name__ == \"__main__\":\n",
    "    staging()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a19896",
   "metadata": {},
   "source": [
    "Primary Stage: Applying the functions created earlier to datasets and handling specific tasks for the police data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ae2fd25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GabriellePye\\AppData\\Local\\Temp\\ipykernel_18044\\2634728761.py:15: DtypeWarning: Columns (17,18,19,20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  datasets[name] = pd.read_csv(file_path)\n",
      "C:\\Users\\GabriellePye\\AppData\\Local\\Temp\\ipykernel_18044\\3964423523.py:16: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df[col] = pd.to_datetime(df[col], errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "def create_primary_folder():\n",
    "    \"\"\"Create the primary folder if it does not exist.\"\"\"\n",
    "    if not os.path.exists(PRIMARY_FOLDER):\n",
    "        os.makedirs(PRIMARY_FOLDER)\n",
    "        logging.info(f\"Created primary folder: {PRIMARY_FOLDER}\")\n",
    "    else:\n",
    "        logging.info(f\"Primary folder already exists: {PRIMARY_FOLDER}\")\n",
    "\n",
    "def load_staged_data():\n",
    "    \"\"\"Load all staged datasets.\"\"\"\n",
    "    datasets = {}\n",
    "    for name, file_name in STAGED_FILES.items():\n",
    "        file_path = os.path.join(STAGING_FOLDER, file_name)\n",
    "        if os.path.exists(file_path):\n",
    "            datasets[name] = pd.read_csv(file_path)\n",
    "            logging.info(f\"Loaded staged dataset: {file_name}\")\n",
    "        else:\n",
    "            logging.warning(f\"Staged file not found: {file_name}\")\n",
    "    return datasets\n",
    "\n",
    "def process_datasets(datasets: dict) -> None:\n",
    "    \"\"\"Process datasets: standardize, handle dates, and perform other operations.\"\"\"\n",
    "    for name, df in datasets.items():\n",
    "        if df is not None:\n",
    "            # Apply standardization\n",
    "            df = standardize_data(df)\n",
    "            \n",
    "            # Drop empty columns if they exist\n",
    "            if df.isna().all().any():\n",
    "                df = drop_empty_columns(df)\n",
    "            \n",
    "            # Handle missing values if there are numerical columns\n",
    "            if df.select_dtypes(include=['number']).columns.size > 0:\n",
    "                df = handle_missing_values(df)\n",
    "            \n",
    "            # Handle dates if date columns are present\n",
    "            if any(col for col in df.columns if 'date' in col.lower()):\n",
    "                df = handle_dates(df)\n",
    "            \n",
    "            # Remove outliers if latitude and longitude columns are present\n",
    "            if 'latitude' in df.columns and 'longitude' in df.columns:\n",
    "                df = remove_outliers(df, lat_range=(50, 52), lon_range=(-2, 2))\n",
    "            \n",
    "            # Remove duplicates if there are any\n",
    "            if df.duplicated().any():\n",
    "                df = remove_duplicates(df)\n",
    "            \n",
    "            # Save processed data to primary folder\n",
    "            primary_file_name = PRIMARY_FILES.get(name)\n",
    "            if primary_file_name:\n",
    "                save_to_csv(df, os.path.join(PRIMARY_FOLDER, primary_file_name))\n",
    "\n",
    "def primary():\n",
    "    \"\"\"Main function for primary processing stage.\"\"\"\n",
    "    create_primary_folder()\n",
    "    staged_data = load_staged_data()\n",
    "    process_datasets(staged_data)\n",
    "    logging.info(\"Primary stage processing completed.\")\n",
    "\n",
    "# Run the primary function\n",
    "if __name__ == \"__main__\":\n",
    "    primary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55016812",
   "metadata": {},
   "source": [
    "Reporting Stage: This will load the primary datasets and add either a postcode if applicable, or save it as a reporting file. This stage will also cover the specific police dataset functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341e17f1",
   "metadata": {},
   "source": [
    "Postcode Function: Uses Longitude and Latitude to add a postcode column to the dataframe. Incorporated with cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "481a07e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cache():\n",
    "    \"\"\"Load cache from a file, handle JSON errors, and return an empty dict if invalid.\"\"\"\n",
    "    if os.path.exists(CACHE_FILE):\n",
    "        try:\n",
    "            with open(CACHE_FILE, 'r') as file:\n",
    "                return json.load(file)\n",
    "        except (json.JSONDecodeError, IOError) as e:\n",
    "            logging.error(f\"Error loading cache file {CACHE_FILE}: {e}\")\n",
    "            return {}\n",
    "    return {}\n",
    "\n",
    "def save_cache(cache):\n",
    "    \"\"\"Save the cache to a file.\"\"\"\n",
    "    try:\n",
    "        with open(CACHE_FILE, 'w') as file:\n",
    "            json.dump(cache, file, indent=4)  # Pretty-print JSON for easier debugging\n",
    "    except IOError as e:\n",
    "        logging.error(f\"Error saving cache file {CACHE_FILE}: {e}\")\n",
    "\n",
    "def get_postcode(lat, lon, cache):\n",
    "    \"\"\"Convert latitude and longitude to postcode using Geopy with caching.\"\"\"\n",
    "    key = f\"{lat},{lon}\"\n",
    "    if key in cache:\n",
    "        return cache[key]\n",
    "    \n",
    "    geolocator = Nominatim(user_agent='Geop_crime_data')\n",
    "    try:\n",
    "        location = geolocator.reverse((lat, lon), exactly_one=True)\n",
    "        address = location.raw.get('address', {})\n",
    "        postcode = address.get('postcode', 'postcode not found')\n",
    "    except Exception as e:\n",
    "        postcode = 'postcode not found'\n",
    "        logging.error(f\"Error getting postcode for lat {lat}, lon {lon}: {e}\")\n",
    "    \n",
    "    cache[key] = postcode\n",
    "    save_cache(cache)\n",
    "    return postcode\n",
    "\n",
    "def add_postcode_column(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add a postcode column to the DataFrame based on latitude and longitude.\"\"\"\n",
    "    if 'latitude' in df.columns and 'longitude' in df.columns:\n",
    "        cache = load_cache()\n",
    "        df['postcode'] = df.apply(\n",
    "            lambda row: get_postcode(row['latitude'], row['longitude'], cache)\n",
    "            if pd.notna(row['latitude']) and pd.notna(row['longitude'])\n",
    "            else 'postcode not found',\n",
    "            axis=1\n",
    "        )\n",
    "        # Optionally drop the latitude and longitude columns if no longer needed\n",
    "        # df.drop(['latitude', 'longitude'], axis=1, inplace=True)\n",
    "    else:\n",
    "        logging.info(\"No latitude and longitude columns found. Skipping postcode conversion.\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c8ea1158",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reporting_folder():\n",
    "    \"\"\"Create the reporting folder if it does not exist.\"\"\"\n",
    "    if not os.path.exists(REPORTING_FOLDER):\n",
    "        os.makedirs(REPORTING_FOLDER)\n",
    "        logging.info(f\"Created reporting folder: {REPORTING_FOLDER}\")\n",
    "    else:\n",
    "        logging.info(f\"Reporting folder already exists: {REPORTING_FOLDER}\")\n",
    "\n",
    "def save_to_csv(df: pd.DataFrame, file_path: str):\n",
    "    \"\"\"Save a DataFrame to a CSV file.\"\"\"\n",
    "    try:\n",
    "        df.to_csv(file_path, index=False)\n",
    "        logging.info(f\"Successfully saved file: {file_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving file {file_path}: {e}\")\n",
    "\n",
    "def process_police_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Process the police dataset with specific transformations.\"\"\"\n",
    "    if 'Month' in df.columns:\n",
    "        # Convert 'Month' to 'Date', drop original 'Month' and extract new columns\n",
    "        df['Date'] = pd.to_datetime(df['Month'], format='%Y-%m', errors='coerce')\n",
    "        df.drop(columns=['Month'], inplace=True)\n",
    "        df['Month'] = df['Date'].dt.strftime('%B')  # 'January', 'February', etc.\n",
    "        df['Year'] = df['Date'].dt.year\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    columns_to_drop = ['Last outcome category', 'Context', 'Falls within']\n",
    "    df.drop(columns=[col for col in columns_to_drop if col in df.columns], inplace=True)\n",
    "    \n",
    "    # Assign IDs for specific crime types\n",
    "    prefix = 'ID_'\n",
    "    if 'Crime type' in df.columns and 'Crime ID' in df.columns:\n",
    "        df.loc[\n",
    "            (df['Crime type'] == 'Anti-social behaviour') & (df['Crime ID'].isna()),\n",
    "            'Crime ID'] = [f'{prefix}{i:03d}' for i in range(1, (df['Crime type'] == 'Anti-social behaviour').sum() + 1)]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_postcode_column(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add a postcode column to the DataFrame based on latitude and longitude.\"\"\"\n",
    "    if 'latitude' in df.columns and 'longitude' in df.columns:\n",
    "        cache = load_cache()\n",
    "        df['postcode'] = df.apply(\n",
    "            lambda row: get_postcode(row['latitude'], row['longitude'], cache)\n",
    "            if pd.notna(row['latitude']) and pd.notna(row['longitude'])\n",
    "            else 'postcode not found',\n",
    "            axis=1\n",
    "        )\n",
    "        # Optionally drop the latitude and longitude columns if no longer needed\n",
    "        # df.drop(['latitude', 'longitude'], axis=1, inplace=True)\n",
    "    else:\n",
    "        logging.info(\"No latitude and longitude columns found. Skipping postcode conversion.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def reporting():\n",
    "    \"\"\"Final reporting stage.\"\"\"\n",
    "    create_reporting_folder()\n",
    "    \n",
    "    # Load primary datasets\n",
    "    primary_files = {\n",
    "        'primary_police_data': 'primary_police_data.csv',\n",
    "        'primary_cbp_8322_authority': 'primary_CBP-8322-authority.csv',\n",
    "        'primary_cbp_8322_constituency': 'primary_CBP-8322-constituency.csv',\n",
    "        'primary_cbp_7293': 'primary_CBP-7293.csv',\n",
    "        'primary_english_la_name_codes': 'primary_EnglishLaNameCodes.csv',\n",
    "        'primary_house_prices': 'primary_house_prices.csv'\n",
    "    }\n",
    "    \n",
    "    for name, file_name in primary_files.items():\n",
    "        file_path = os.path.join(PRIMARY_FOLDER, file_name)\n",
    "        if os.path.exists(file_path):\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            if name == 'primary_police_data':\n",
    "                df = process_police_data(df)\n",
    "                df = add_postcode_column(df)  # Add postcode column if applicable\n",
    "            \n",
    "            # Save the processed DataFrame to the reporting folder\n",
    "            save_to_csv(df, os.path.join(REPORTING_FOLDER, file_name))\n",
    "        else:\n",
    "            logging.warning(f\"Primary file not found: {file_name}\")\n",
    "\n",
    "# Run the reporting function\n",
    "if __name__ == \"__main__\":\n",
    "    reporting()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fae51d",
   "metadata": {},
   "source": [
    "Now all together: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6b837d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(pipeline='all'):\n",
    "    logging.info(\"Pipeline execution started\")\n",
    "\n",
    "    try:\n",
    "        if pipeline in ['all', 'staging', 'primary', 'reporting']:\n",
    "            if pipeline in ['all', 'staging']:\n",
    "                staging()\n",
    "                logging.info(\"Staging execution completed successfully\")\n",
    "                \n",
    "            if pipeline in ['all', 'primary']:\n",
    "                primary()\n",
    "                logging.info(\"Primary execution completed successfully\")\n",
    "                \n",
    "            if pipeline in ['all', 'reporting']:\n",
    "                reporting()\n",
    "                logging.info(\"Reporting execution completed successfully\")\n",
    "                \n",
    "            logging.info(\"Pipeline run complete\")\n",
    "        \n",
    "        else:\n",
    "            # Inform the user about an invalid pipeline stage input\n",
    "            logging.critical(\"Invalid pipeline stage specified. Please choose 'staging', 'primary', 'reporting', or 'all'.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Catch and print any exceptions occurred during pipeline execution\n",
    "        logging.error(f\"Pipeline execution failed: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
