{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16d1f10c-cd7e-4c56-bc46-8fc1838d1553",
   "metadata": {},
   "source": [
    "# Data Processing Pipeline\n",
    "\n",
    "## Overview\n",
    "\n",
    "This pipeline performs a series of data processing stages to handle and transform datasets. The pipeline is designed to be modular and can execute specific stages independently or run the entire pipeline sequentially.\n",
    "\n",
    "## Stages\n",
    "\n",
    "1. **Staging**\n",
    "   - Loads raw datasets from the source directory.\n",
    "   - Standardizes column names and formats.\n",
    "   - Saves standardized datasets to a staging folder.\n",
    "\n",
    "2. **Primary Processing**\n",
    "   - Loads datasets from the staging folder.\n",
    "   - Applies transformations including data cleaning, standardization, and additional calculations.\n",
    "   - Specifically processes police data with transformations such as handling missing values, creating new columns, and applying specific rules.\n",
    "   - Saves processed datasets to a primary folder.\n",
    "\n",
    "3. **Reporting**\n",
    "   - Loads datasets from the primary folder.\n",
    "   - Applies any additional transformations or updates required for reporting.\n",
    "   - Specifically processes police data to add a `Postcode` column based on latitude and longitude.\n",
    "   - Saves the final datasets to a reporting folder.\n",
    "\n",
    "## Functions\n",
    "\n",
    "- **`create_staging_folder()`**: Creates the staging folder if it does not exist.\n",
    "- **`save_to_csv(df, file_path)`**: Saves a DataFrame to a CSV file.\n",
    "- **`process_police_data(df)`**: Applies specific transformations to the police dataset.\n",
    "- **`add_postcode_column(df)`**: Adds a postcode column to the DataFrame based on latitude and longitude.\n",
    "- **`reporting()`**: Final stage that processes and saves datasets to the reporting folder.\n",
    "- **`main(pipeline='all')`**: Executes the specified pipeline stages based on user input.\n",
    "\n",
    "## Usage\n",
    "\n",
    "To run the pipeline, execute the script with the desired pipeline stage(s):\n",
    "\n",
    "- `Completed Python Pipeline.py` to run the entire pipeline.\n",
    "- `Completed Python Pipeline.py staging` to run only the staging stage.\n",
    "- `Completed Python Pipeline.py primary` to run only the primary processing stage.\n",
    "- `Completed Python Pipeline.py reporting` to run only the reporting stage.\n",
    "\n",
    "## Caching\n",
    "\n",
    "The postcode retrieval process uses caching to reduce the number of API calls and improve performance. Cached results are stored in `postcode_cache.json`.\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "- `pandas`\n",
    "- `geopy`\n",
    "- `logging`\n",
    "- `os`\n",
    "- `json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1115f85e-c5f6-4188-9dc9-acfeaa921f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from geopy.geocoders import Nominatim\n",
    "import json\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, # Levels above debug\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    filename='data_pipeline.log', # Pipeline log file\n",
    "    filemode='w' # Overwrite logs\n",
    ")\n",
    "\n",
    "# Initialize geocoder\n",
    "geolocator = Nominatim(user_agent='myGeocoder')\n",
    "\n",
    "# Constants\n",
    "DATA_FOLDER = './Data/' # Paths to each dataset file and their types are defined.\n",
    "STAGING_FOLDER = './staging'\n",
    "PRIMARY_FOLDER = './primary'\n",
    "POLICE_DATA_FOLDER = os.path.join(DATA_FOLDER, 'Police Dataset 2021-2024')\n",
    "REPORTING_FOLDER = './reporting'\n",
    "CACHE_FILE = 'postcode_cache.json'\n",
    "\n",
    "STAGED_FILES = {\n",
    "    'staged_police_data': 'staged_police_data.csv',\n",
    "    'staged_cbp_8322_authority': 'staged_CBP-8322-authority.csv',\n",
    "    'staged_cbp_8322_constituency': 'staged_CBP-8322-constituency.csv',\n",
    "    'staged_cbp_7293': 'staged_CBP-7293.csv',\n",
    "    'staged_english_la_name_codes': 'staged_EnglishLaNameCodes.csv',\n",
    "    'staged_house_prices': 'staged_house_prices.csv'\n",
    "}\n",
    "\n",
    "PRIMARY_FILES = {\n",
    "    'staged_police_data': 'primary_police_data.csv',\n",
    "    'staged_cbp_8322_authority': 'primary_CBP-8322-authority.csv',\n",
    "    'staged_cbp_8322_constituency': 'primary_CBP-8322-constituency.csv',\n",
    "    'staged_cbp_7293': 'primary_CBP-7293.csv',\n",
    "    'staged_english_la_name_codes': 'primary_EnglishLaNameCodes.csv',\n",
    "    'staged_house_prices': 'primary_house_prices.csv'\n",
    "}\n",
    "\n",
    "REPORTING_FILES = {\n",
    "    'primary_police_data': 'reporting_police_data.csv',\n",
    "    'primary_CBP-8322-authority': 'reporting_CBP-8322-authority.csv',\n",
    "    'primary_CBP-8322-constituency': 'reporting_CBP-8322-constituency.csv',\n",
    "    'primary_CBP-7293': 'reporting_CBP-7293.csv',\n",
    "    'primary_EnglishLaNameCodes': 'reporting_EnglishLaNameCodes.csv',\n",
    "    'primary_house_prices': 'reporting_house_prices.csv'\n",
    "}\n",
    "\n",
    "\n",
    "# Define dataset file paths and types\n",
    "CBP_8322_AUTHORITY_FILE = os.path.join(DATA_FOLDER, 'CBP-8322-authority.xlsx')\n",
    "CBP_8322_CONSTITUENCY_FILE = os.path.join(DATA_FOLDER, 'CBP-8322-constituency.xlsx')\n",
    "CBP_7293_FILE = os.path.join(DATA_FOLDER, 'CBP-7293.xlsx')\n",
    "ENGLISH_LA_NAME_CODES_FILE = os.path.join(DATA_FOLDER, 'EnglishLaNameCodes.csv')\n",
    "HOUSE_PRICES_FILE = os.path.join(DATA_FOLDER, 'house_prices.csv')\n",
    "POLICE_DATA_FOLDER = os.path.join(DATA_FOLDER, 'Police Dataset 2021-2024')\n",
    "\n",
    "# Define file types\n",
    "FILE_TYPES = {\n",
    "    'CBP-8322-authority': 'excel',\n",
    "    'CBP-8322-constituency': 'excel',\n",
    "    'CBP-7293': 'excel',\n",
    "    'EnglishLaNameCodes': 'csv',\n",
    "    'house_prices': 'csv'\n",
    "}\n",
    "\n",
    "def load_data(file_path: str, file_type: str) -> pd.DataFrame: # Loads individual datasets based on file typ\n",
    "    \"\"\"Load a dataset from a given file path and type (csv or excel).\"\"\"\n",
    "    try:\n",
    "        if file_type == 'csv':\n",
    "            df = pd.read_csv(file_path)\n",
    "        elif file_type == 'excel':\n",
    "            df = pd.read_excel(file_path)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file type. Only 'csv' and 'excel' are supported.\")\n",
    "        logging.info(f\"Successfully loaded {file_type} file: {file_path}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading {file_type} file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_police_data(police_folder_path: str) -> pd.DataFrame: # Concatenates multiple police datasets into one DataFrame\n",
    "    \"\"\"Load and concatenate police datasets.\"\"\"\n",
    "    df_list = []\n",
    "    for root, dirs, files in os.walk(police_folder_path):\n",
    "        for file_name in files:\n",
    "            if file_name.endswith('.csv'):\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                df = load_data(file_path, 'csv')\n",
    "                if df is not None:\n",
    "                    df_list.append(df)\n",
    "    \n",
    "    if df_list:\n",
    "        combined_df = pd.concat(df_list, ignore_index=True)\n",
    "        logging.info(\"Successfully concatenated police datasets.\")\n",
    "        return combined_df\n",
    "    else:\n",
    "        logging.error(\"No valid police datasets found to concatenate.\")\n",
    "        return None\n",
    "\n",
    "def load_all_datasets(): # Uses constants to load all datasets and handle the police data separately.\n",
    "    \"\"\"Load all datasets using predefined constants.\"\"\"\n",
    "    files_and_types = {\n",
    "        'CBP-8322-authority': (CBP_8322_AUTHORITY_FILE, FILE_TYPES['CBP-8322-authority']),\n",
    "        'CBP-8322-constituency': (CBP_8322_CONSTITUENCY_FILE, FILE_TYPES['CBP-8322-constituency']),\n",
    "        'CBP-7293': (CBP_7293_FILE, FILE_TYPES['CBP-7293']),\n",
    "        'EnglishLaNameCodes': (ENGLISH_LA_NAME_CODES_FILE, FILE_TYPES['EnglishLaNameCodes']),\n",
    "        'house_prices': (HOUSE_PRICES_FILE, FILE_TYPES['house_prices'])\n",
    "    }\n",
    "    \n",
    "    datasets = {}\n",
    "    for name, (file_path, file_type) in files_and_types.items():\n",
    "        df = load_data(file_path, file_type)\n",
    "        if df is not None:\n",
    "            datasets[name] = df\n",
    "    \n",
    "    # Load police data separately\n",
    "    police_df = load_police_data(POLICE_DATA_FOLDER)\n",
    "    if police_df is not None:\n",
    "        datasets['police'] = police_df\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "# FUNCTIONS \n",
    "\n",
    "def standardize_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Standardize text columns by converting to lowercase and stripping whitespace.\"\"\"\n",
    "    \n",
    "    # Normalize column names (lowercase and replace spaces with underscores)\n",
    "    df.columns = [col.lower().replace(' ', '_') for col in df.columns]\n",
    "    \n",
    "    # Identify object (string) columns dynamically\n",
    "    string_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    # If there are no string columns, log and skip processing\n",
    "    if not string_columns:\n",
    "        logging.info(\"No string columns found for standardization.\")\n",
    "        return df\n",
    "    \n",
    "    # Standardize string columns (convert to lowercase, strip whitespace)\n",
    "    for col in string_columns:\n",
    "        logging.info(f\"Standardizing column: {col}\")\n",
    "        df[col] = df[col].astype(str).str.lower().str.strip()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def remove_duplicates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Remove duplicate rows based on all columns if duplicates exist.\"\"\"\n",
    "    try:\n",
    "        # Check for duplicates\n",
    "        if df.duplicated().any():\n",
    "            # Drop duplicate rows across all columns\n",
    "            df = df.drop_duplicates()\n",
    "            logging.info(\"Duplicates removed successfully.\")\n",
    "        else:\n",
    "            logging.info(\"No duplicates found; skipping removal.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error removing duplicates: {e}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def handle_dates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Convert columns with date-like data into datetime and create month/year columns if applicable.\"\"\"\n",
    "    \n",
    "    # Identify date columns: columns that contain 'date' in their name or have date-like values\n",
    "    date_columns = [col for col in df.columns if 'date' in col.lower() or df[col].astype(str).str.contains(r'\\d{2}/\\d{2}/\\d{4}', na=False).any()]\n",
    "    \n",
    "    # If no date columns are found, log and return the DataFrame unchanged\n",
    "    if not date_columns:\n",
    "        logging.info(\"No date-like columns found.\")\n",
    "        return df\n",
    "    \n",
    "    logging.info(f\"Converting date columns: {date_columns}\")\n",
    "    \n",
    "    for col in date_columns:\n",
    "        # Convert column to datetime\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "\n",
    "        # Create 'month' and 'year' columns based on the datetime column\n",
    "        df[f'{col}_month'] = df[col].dt.month\n",
    "        df[f'{col}_year'] = df[col].dt.year\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def drop_empty_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Drop columns that are entirely empty if any exist.\"\"\"\n",
    "    # Identify columns that are entirely empty\n",
    "    empty_cols = df.columns[df.isna().all()].tolist()\n",
    "    \n",
    "    if empty_cols:\n",
    "        logging.info(f\"Dropping empty columns: {empty_cols}\")\n",
    "        df = df.drop(columns=empty_cols)\n",
    "    else:\n",
    "        logging.info(\"No empty columns to drop.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def handle_missing_values(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Handle missing values by filling numerical columns with median values if necessary.\"\"\"\n",
    "    # Identify numerical columns\n",
    "    numerical_cols = df.select_dtypes(include=['number']).columns\n",
    "    \n",
    "    # Check if there are any numerical columns with missing values\n",
    "    missing_data = df[numerical_cols].isna().any().any()\n",
    "    \n",
    "    if not missing_data:\n",
    "        # If no missing values are found in numerical columns, skip processing\n",
    "        logging.info(\"No missing values found in numerical columns. Skipping missing values handling.\")\n",
    "        return df\n",
    "    \n",
    "    # If there are missing values, handle them\n",
    "    for col in numerical_cols:\n",
    "        if df[col].isna().any():\n",
    "            logging.info(f\"Handling missing values for column: {col}\")\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "    \n",
    "    return df\n",
    "\n",
    "def remove_outliers(df: pd.DataFrame, lat_range: tuple = (50, 52), lon_range: tuple = (-2, 2)) -> pd.DataFrame:\n",
    "    \"\"\"Remove outliers based on latitude and longitude ranges if necessary.\"\"\"\n",
    "    if 'latitude' in df.columns and 'longitude' in df.columns:\n",
    "        # Check if any latitude or longitude values are outside the specified ranges\n",
    "        outlier_lat = df['latitude'].notna() & ~df['latitude'].between(lat_range[0], lat_range[1])\n",
    "        outlier_lon = df['longitude'].notna() & ~df['longitude'].between(lon_range[0], lon_range[1])\n",
    "        \n",
    "        if outlier_lat.any() or outlier_lon.any():\n",
    "            logging.info(\"Removing outliers based on latitude and longitude.\")\n",
    "            original_size = len(df)\n",
    "            df = df[~(outlier_lat | outlier_lon)]\n",
    "            new_size = len(df)\n",
    "            logging.info(f\"Removed {original_size - new_size} rows with out-of-range lat/long values.\")\n",
    "        else:\n",
    "            logging.info(\"No outliers found in latitude/longitude columns. Skipping outlier removal.\")\n",
    "    else:\n",
    "        logging.info(\"No latitude/longitude columns found. Skipping outlier removal.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# STAGING\n",
    "\n",
    "def create_staging_folder():\n",
    "    \"\"\"Create the staging folder if it does not exist.\"\"\"\n",
    "    if not os.path.exists(STAGING_FOLDER):\n",
    "        os.makedirs(STAGING_FOLDER)\n",
    "        logging.info(f\"Created staging folder: {STAGING_FOLDER}\")\n",
    "    else:\n",
    "        logging.info(f\"Staging folder already exists: {STAGING_FOLDER}\")\n",
    "\n",
    "def load_data(file_path: str, file_type: str) -> pd.DataFrame:\n",
    "    \"\"\"Load a dataset from a given file path and type (csv or excel).\"\"\"\n",
    "    try:\n",
    "        if file_type == 'csv':\n",
    "            df = pd.read_csv(file_path)\n",
    "        elif file_type == 'excel':\n",
    "            df = pd.read_excel(file_path)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file type. Only 'csv' and 'excel' are supported.\")\n",
    "        logging.info(f\"Successfully loaded {file_type} file: {file_path}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading {file_type} file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_police_data(police_folder_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load and concatenate police datasets.\"\"\"\n",
    "    df_list = []\n",
    "    for root, dirs, files in os.walk(police_folder_path):\n",
    "        for file_name in files:\n",
    "            if file_name.endswith('.csv'):\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                df = load_data(file_path, 'csv')\n",
    "                if df is not None:\n",
    "                    df_list.append(df)\n",
    "    \n",
    "    if df_list:\n",
    "        combined_df = pd.concat(df_list, ignore_index=True)\n",
    "        logging.info(\"Successfully concatenated police datasets.\")\n",
    "        return combined_df\n",
    "    else:\n",
    "        logging.error(\"No valid police datasets found to concatenate.\")\n",
    "        return None\n",
    "\n",
    "def save_to_csv(df: pd.DataFrame, file_path: str):\n",
    "    \"\"\"Save a DataFrame to a CSV file.\"\"\"\n",
    "    try:\n",
    "        df.to_csv(file_path, index=False)\n",
    "        logging.info(f\"Successfully saved file: {file_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving file {file_path}: {e}\")\n",
    "\n",
    "def staging():\n",
    "    \"\"\"Load datasets, concatenate police data, and save as staged files.\"\"\"\n",
    "    create_staging_folder()\n",
    "    \n",
    "    # Load police data\n",
    "    police_df = load_police_data(POLICE_DATA_FOLDER)\n",
    "    if police_df is not None:\n",
    "        save_to_csv(police_df, os.path.join(STAGING_FOLDER, 'staged_police_data.csv'))\n",
    "\n",
    "    # Load other datasets\n",
    "    files_and_types = {\n",
    "        'CBP-8322-authority': (CBP_8322_AUTHORITY_FILE, FILE_TYPES['CBP-8322-authority']),\n",
    "        'CBP-8322-constituency': (CBP_8322_CONSTITUENCY_FILE, FILE_TYPES['CBP-8322-constituency']),\n",
    "        'CBP-7293': (CBP_7293_FILE, FILE_TYPES['CBP-7293']),\n",
    "        'EnglishLaNameCodes': (ENGLISH_LA_NAME_CODES_FILE, FILE_TYPES['EnglishLaNameCodes']),\n",
    "        'house_prices': (HOUSE_PRICES_FILE, FILE_TYPES['house_prices'])\n",
    "    }\n",
    "    \n",
    "    for name, (file_path, file_type) in files_and_types.items():\n",
    "        df = load_data(file_path, file_type)\n",
    "        if df is not None:\n",
    "            save_to_csv(df, os.path.join(STAGING_FOLDER, f'staged_{name}.csv'))\n",
    "\n",
    "# Run the staging function\n",
    "if __name__ == \"__main__\":\n",
    "    staging()\n",
    "\n",
    "# PRIMARY\n",
    "\n",
    "def create_primary_folder():\n",
    "    \"\"\"Create the primary folder if it does not exist.\"\"\"\n",
    "    if not os.path.exists(PRIMARY_FOLDER):\n",
    "        os.makedirs(PRIMARY_FOLDER)\n",
    "        logging.info(f\"Created primary folder: {PRIMARY_FOLDER}\")\n",
    "    else:\n",
    "        logging.info(f\"Primary folder already exists: {PRIMARY_FOLDER}\")\n",
    "\n",
    "def load_staged_data():\n",
    "    \"\"\"Load all staged datasets.\"\"\"\n",
    "    datasets = {}\n",
    "    for name, file_name in STAGED_FILES.items():\n",
    "        file_path = os.path.join(STAGING_FOLDER, file_name)\n",
    "        if os.path.exists(file_path):\n",
    "            datasets[name] = pd.read_csv(file_path)\n",
    "            logging.info(f\"Loaded staged dataset: {file_name}\")\n",
    "        else:\n",
    "            logging.warning(f\"Staged file not found: {file_name}\")\n",
    "    return datasets\n",
    "\n",
    "def process_datasets(datasets: dict) -> None:\n",
    "    \"\"\"Process datasets: standardize, handle dates, and perform other operations.\"\"\"\n",
    "    for name, df in datasets.items():\n",
    "        if df is not None:\n",
    "            # Apply standardization\n",
    "            df = standardize_data(df)\n",
    "            \n",
    "            # Drop empty columns if they exist\n",
    "            if df.isna().all().any():\n",
    "                df = drop_empty_columns(df)\n",
    "            \n",
    "            # Handle missing values if there are numerical columns\n",
    "            if df.select_dtypes(include=['number']).columns.size > 0:\n",
    "                df = handle_missing_values(df)\n",
    "            \n",
    "            # Handle dates if date columns are present\n",
    "            if any(col for col in df.columns if 'date' in col.lower()):\n",
    "                df = handle_dates(df)\n",
    "            \n",
    "            # Remove outliers if latitude and longitude columns are present\n",
    "            if 'latitude' in df.columns and 'longitude' in df.columns:\n",
    "                df = remove_outliers(df, lat_range=(50, 52), lon_range=(-2, 2))\n",
    "            \n",
    "            # Remove duplicates if there are any\n",
    "            if df.duplicated().any():\n",
    "                df = remove_duplicates(df)\n",
    "            \n",
    "            # Save processed data to primary folder\n",
    "            primary_file_name = PRIMARY_FILES.get(name)\n",
    "            if primary_file_name:\n",
    "                save_to_csv(df, os.path.join(PRIMARY_FOLDER, primary_file_name))\n",
    "\n",
    "def primary():\n",
    "    \"\"\"Main function for primary processing stage.\"\"\"\n",
    "    create_primary_folder()\n",
    "    staged_data = load_staged_data()\n",
    "    process_datasets(staged_data)\n",
    "    logging.info(\"Primary stage processing completed.\")\n",
    "\n",
    "# Run the primary function\n",
    "if __name__ == \"__main__\":\n",
    "    primary()\n",
    "\n",
    "# REPORTING FUNCTIONS \n",
    "\n",
    "def load_cache():\n",
    "    \"\"\"Load cache from a file, handle JSON errors, and return an empty dict if invalid.\"\"\"\n",
    "    if os.path.exists(CACHE_FILE):\n",
    "        try:\n",
    "            with open(CACHE_FILE, 'r') as file:\n",
    "                return json.load(file)\n",
    "        except (json.JSONDecodeError, IOError) as e:\n",
    "            logging.error(f\"Error loading cache file {CACHE_FILE}: {e}\")\n",
    "            return {}\n",
    "    return {}\n",
    "\n",
    "def save_cache(cache):\n",
    "    \"\"\"Save the cache to a file.\"\"\"\n",
    "    try:\n",
    "        with open(CACHE_FILE, 'w') as file:\n",
    "            json.dump(cache, file, indent=4)  # Pretty-print JSON for easier debugging\n",
    "    except IOError as e:\n",
    "        logging.error(f\"Error saving cache file {CACHE_FILE}: {e}\")\n",
    "\n",
    "def get_postcode(lat, lon, cache):\n",
    "    \"\"\"Convert latitude and longitude to postcode using Geopy with caching.\"\"\"\n",
    "    key = f\"{lat},{lon}\"\n",
    "    if key in cache:\n",
    "        return cache[key]\n",
    "    \n",
    "    geolocator = Nominatim(user_agent='Geop_crime_data')\n",
    "    try:\n",
    "        location = geolocator.reverse((lat, lon), exactly_one=True)\n",
    "        address = location.raw.get('address', {})\n",
    "        postcode = address.get('postcode', 'postcode not found')\n",
    "    except Exception as e:\n",
    "        postcode = 'postcode not found'\n",
    "        logging.error(f\"Error getting postcode for lat {lat}, lon {lon}: {e}\")\n",
    "    \n",
    "    cache[key] = postcode\n",
    "    save_cache(cache)\n",
    "    return postcode\n",
    "\n",
    "def add_postcode_column(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add a postcode column to the DataFrame based on latitude and longitude.\"\"\"\n",
    "    if 'latitude' in df.columns and 'longitude' in df.columns:\n",
    "        cache = load_cache()\n",
    "        df['postcode'] = df.apply(\n",
    "            lambda row: get_postcode(row['latitude'], row['longitude'], cache)\n",
    "            if pd.notna(row['latitude']) and pd.notna(row['longitude'])\n",
    "            else 'postcode not found',\n",
    "            axis=1\n",
    "        )\n",
    "        # Optionally drop the latitude and longitude columns if no longer needed\n",
    "        # df.drop(['latitude', 'longitude'], axis=1, inplace=True)\n",
    "    else:\n",
    "        logging.info(\"No latitude and longitude columns found. Skipping postcode conversion.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# REPORTING \n",
    "\n",
    "def create_reporting_folder():\n",
    "    \"\"\"Create the reporting folder if it does not exist.\"\"\"\n",
    "    if not os.path.exists(REPORTING_FOLDER):\n",
    "        os.makedirs(REPORTING_FOLDER)\n",
    "        logging.info(f\"Created reporting folder: {REPORTING_FOLDER}\")\n",
    "    else:\n",
    "        logging.info(f\"Reporting folder already exists: {REPORTING_FOLDER}\")\n",
    "\n",
    "def save_to_csv(df: pd.DataFrame, file_path: str):\n",
    "    \"\"\"Save a DataFrame to a CSV file.\"\"\"\n",
    "    try:\n",
    "        df.to_csv(file_path, index=False)\n",
    "        logging.info(f\"Successfully saved file: {file_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving file {file_path}: {e}\")\n",
    "\n",
    "def process_police_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Process the police dataset with specific transformations.\"\"\"\n",
    "    if 'Month' in df.columns:\n",
    "        # Convert 'Month' to 'Date', drop original 'Month' and extract new columns\n",
    "        df['Date'] = pd.to_datetime(df['Month'], format='%Y-%m', errors='coerce')\n",
    "        df.drop(columns=['Month'], inplace=True)\n",
    "        df['Month'] = df['Date'].dt.strftime('%B')  # 'January', 'February', etc.\n",
    "        df['Year'] = df['Date'].dt.year\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    columns_to_drop = ['Last outcome category', 'Context', 'Falls within']\n",
    "    df.drop(columns=[col for col in columns_to_drop if col in df.columns], inplace=True)\n",
    "    \n",
    "    # Assign IDs for specific crime types\n",
    "    prefix = 'ID_'\n",
    "    if 'Crime type' in df.columns and 'Crime ID' in df.columns:\n",
    "        df.loc[\n",
    "            (df['Crime type'] == 'Anti-social behaviour') & (df['Crime ID'].isna()),\n",
    "            'Crime ID'] = [f'{prefix}{i:03d}' for i in range(1, (df['Crime type'] == 'Anti-social behaviour').sum() + 1)]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_postcode_column(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add a postcode column to the DataFrame based on latitude and longitude.\"\"\"\n",
    "    if 'latitude' in df.columns and 'longitude' in df.columns:\n",
    "        cache = load_cache()\n",
    "        df['postcode'] = df.apply(\n",
    "            lambda row: get_postcode(row['latitude'], row['longitude'], cache)\n",
    "            if pd.notna(row['latitude']) and pd.notna(row['longitude'])\n",
    "            else 'postcode not found',\n",
    "            axis=1\n",
    "        )\n",
    "        # Optionally drop the latitude and longitude columns if no longer needed\n",
    "        # df.drop(['latitude', 'longitude'], axis=1, inplace=True)\n",
    "    else:\n",
    "        logging.info(\"No latitude and longitude columns found. Skipping postcode conversion.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def reporting():\n",
    "    \"\"\"Final reporting stage.\"\"\"\n",
    "    create_reporting_folder()\n",
    "    \n",
    "    # Load primary datasets\n",
    "    primary_files = {\n",
    "        'primary_police_data': 'primary_police_data.csv',\n",
    "        'primary_cbp_8322_authority': 'primary_CBP-8322-authority.csv',\n",
    "        'primary_cbp_8322_constituency': 'primary_CBP-8322-constituency.csv',\n",
    "        'primary_cbp_7293': 'primary_CBP-7293.csv',\n",
    "        'primary_english_la_name_codes': 'primary_EnglishLaNameCodes.csv',\n",
    "        'primary_house_prices': 'primary_house_prices.csv'\n",
    "    }\n",
    "    \n",
    "    for name, file_name in primary_files.items():\n",
    "        file_path = os.path.join(PRIMARY_FOLDER, file_name)\n",
    "        if os.path.exists(file_path):\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            if name == 'primary_police_data':\n",
    "                df = process_police_data(df)\n",
    "                df = add_postcode_column(df)  # Add postcode column if applicable\n",
    "            \n",
    "            # Save the processed DataFrame to the reporting folder\n",
    "            save_to_csv(df, os.path.join(REPORTING_FOLDER, file_name))\n",
    "        else:\n",
    "            logging.warning(f\"Primary file not found: {file_name}\")\n",
    "\n",
    "# Run the reporting function\n",
    "if __name__ == \"__main__\":\n",
    "    reporting()\n",
    "\n",
    "# To execute pipeline all at once \n",
    "\n",
    "def main(pipeline='all'):\n",
    "    logging.info(\"Pipeline execution started\")\n",
    "\n",
    "    try:\n",
    "        if pipeline in ['all', 'staging', 'primary', 'reporting']:\n",
    "            if pipeline in ['all', 'staging']:\n",
    "                staging()\n",
    "                logging.info(\"Staging execution completed successfully\")\n",
    "                \n",
    "            if pipeline in ['all', 'primary']:\n",
    "                primary()\n",
    "                logging.info(\"Primary execution completed successfully\")\n",
    "                \n",
    "            if pipeline in ['all', 'reporting']:\n",
    "                reporting()\n",
    "                logging.info(\"Reporting execution completed successfully\")\n",
    "                \n",
    "            logging.info(\"Pipeline run complete\")\n",
    "        \n",
    "        else:\n",
    "            # Inform the user about an invalid pipeline stage input\n",
    "            logging.critical(\"Invalid pipeline stage specified. Please choose 'staging', 'primary', 'reporting', or 'all'.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Catch and print any exceptions occurred during pipeline execution\n",
    "        logging.error(f\"Pipeline execution failed: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
